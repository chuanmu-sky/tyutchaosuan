import torch
import numpy as np
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
'''----------------------------------------------------------------------------------------------'''
# x的形状是(6595, 10, 48, 1)
# y的形状是(6595, 10, 12, 1)
# 10组数据，每组数据x的时间序列是48，每个时间点输入的是(6595, 1)
# 每组数据y的形状是(6595, 12), 表示基于前48个时间点所预测出的y, 这里暂时多加一个维度1
# 这里可以用mini_batch操作6595
train_data = np.load("D:\\develop\\data_train\\lstm\\Data\\train.npz")
train_x = torch.tensor(train_data["x"]).transpose(1, 2)
train_y = torch.tensor(train_data["y"]).transpose(1, 2)
dataset = torch.cat((train_x, train_y),dim=2)
train_data_set = DataLoader(dataset=dataset, batch_size=128, shuffle=True)

input_size = 1
hidden_size = 50
output_size = 12
num_layers = 2
dropout = 0.03

node = 10
all_batch = 6595
lr = 0.03
'''----------------------------------------------------------------------------------------------'''
class LSTMModel(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
        super(LSTMModel, self).__init__()
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout)
        self.fc_0 = torch.nn.Linear(hidden_size, 64)
        self.fc_1 = torch.nn.Linear(64, output_size)

    def forward(self, x):
        # x的形状是(n, batch, input_size)
        # out的形状是(n, batch, input_size)
        out, _ = self.lstm(x)
        # 此时取最后一个时间序列 (batch, input_size)
        out = out[-1, :, :]
        # 最终输出 (batch, output_size)
        out = self.fc_1(self.fc_0(out))
        return out

model = list(
     [LSTMModel(input_size, hidden_size, num_layers, output_size, dropout).to(device) 
      for i in range(node)])
'''----------------------------------------------------------------------------------------------'''
criterion = list([torch.nn.MSELoss()
                  for i in range(node)])
optimizer = list(
     [torch.optim.Adam(model[i].parameters(), lr)
      for i in range(node)])
'''----------------------------------------------------------------------------------------------'''
loss_all_all = []
for epoch in range(10):
    loss_all = [0,0,0,0,0,0,0,0,0,0]
    for _, data in enumerate(train_data_set, 0):
        # x是(batch, 10, 48, 1)
        # y是(batch, 10, 12, 1)
        x, y = data[:,:,0:48,:], data[:,:,48:60,:] 
        # x是(10, 48, batch, 1)
        # y是(10, batch, 12)
        x = x.transpose(0, 1).contiguous().view(10, 48, -1, 1).to(device)
        y = y.transpose(0, 1).contiguous().view(10, -1, 12).to(device)

        for i in range(node):
            pred_y = model[i](x[i])
            optimizer[i].zero_grad()
            loss = criterion[i](pred_y, y[i])
            loss.backward()
            optimizer[i].step()

            loss_all[i] = loss

    loss_all_all.append(loss_all)
    print('Epoch: {}'.format(epoch))
    for i in range(node):
        print('Node: {}, Train Loss: {:.5f}'.format(i+1, loss_all[i]))
    
loss_all_all = torch.tensor(loss_all_all).transpose(0, 1)
print(loss_all_all.shape)
for i in range(len(loss_all_all)):
    y = loss_all_all[i]
    plt.plot(y.numpy())
plt.show()
